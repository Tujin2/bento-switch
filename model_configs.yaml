default_model: Nymeria-15B-Q8
models:
  Codestral-22B-v0.1:
    type: llama
    path: "c:/models/bartowski/Codestral-22B-v0.1-GGUF/Codestral-22B-v0.1-Q6_K.gguf"
    prompt_template: "<s> [INST]{system_prompt}{conversation_history} [/INST] </s>"
    system_message_template: "<<SYS>>\n{system_prompt}\n<</SYS>>\n\n"
    default_params:
      temperature: 0.7
      max_tokens: 2000
      top_p: 0.8
      top_k: 40
      stream: True
    n_context: 17000
    n_gpu_layers: -1
  Nymeria-15B-Q8:
    type: llama
    path: "c:/models/mradermacher/L3-Nymeria-15B-GGUF/L3-Nymeria-15B.Q8_0.gguf"
    prompt_template: "{system_prompt}{conversation_history}\n\n<|start_header_id|>assistant<|end_header_id|>"
    system_message_template: "<|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|>\n\n"
    conversation_message_template: "<|start_header_id|>{role}<|end_header_id|>{content}<|eot_id|>\n\n"
    default_params:
      temperature: 0.9
      max_tokens: 2000
      top_p: 0.75
      top_k: 30
      stream: True
    n_context: 8192
    n_gpu_layers: -1
  # Add more models here as needed